{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "dirname = os.path.dirname('__file__')\n",
    "\n",
    "sys.path.append(os.path.join(dirname, \"pkg\"))\n",
    "sys.path.append(os.path.join(dirname, \"pkg\", \"preproc\"))\n",
    "from pkg.preproc import base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectives\n",
    "\n",
    "1. Identify number of potential speakers (clustering)\n",
    "    1.1 Use Factors: Readability (F-K), doc_length, n_mentions, n_hashtags, time_of_day??, avg_word_size, avg_num_syllables\n",
    "2. Attempt to identify Trump (with 'known' data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17433/17433 [06:24<00:00, 45.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Error: \n",
      "4746) Iran humiliated the United States with the capture of our 10 sailors. Horrible pictures and images. We are weak. I will NOT forget!\n",
      "Length Error: \n",
      "4748) Iran humiliated the United States with the capture of our 10 sailors. Horrible pictures and images. We are weak. I will NOT forget!\n"
     ]
    }
   ],
   "source": [
    "from pkg.mod import cluster\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pandas import DataFrame\n",
    "fields = [\"n_hashtags\", \"n_mentions\", \"avg_syllables\",\n",
    "          \"avg_word_length\", \"fk\", 'n_sents',\n",
    "          \"n_ents\", \"n_uppers\", \"platform_id\",'amplifier',\n",
    "          'analneg', 'attribadj', 'auxdo', 'bemv',\n",
    "          'bracket', 'caps', 'cconj', 'cntrstconj',\n",
    "          'colon', 'comma', 'defart', 'detquan',\n",
    "          'exclam', 'fstpp', 'fulstop', 'gerund',\n",
    "          'havemv', 'imperative', 'indefart',\n",
    "          'infinitive', 'it', 'mdnec',\n",
    "          'mdposs', 'mdpred', 'multiwvb',\n",
    "          'nomin', 'numdet', 'numnoun', 'objpro',\n",
    "          'otheradv', 'othrintj', 'othrnoun',\n",
    "          'othrverb', 'passive', 'past', 'perceptvb',\n",
    "          'perfect', 'posesprpn', 'possdet', 'predadj',\n",
    "          'prep', 'procontract', 'progressive',\n",
    "          'proquan', 'provdo', 'prpn', 'prvv', 'pubv',\n",
    "          'ques', 'relclausesubgap', 'sinflect',\n",
    "          'sndpp', 'stancevb', 'subjpro', 'superlative',\n",
    "          'thrdpp', 'timeadv', 'whw', 'initialmention']\n",
    "dat = base.Data()\n",
    "prep = base.Preprocessor()\n",
    "dat.get_data(load_local=True)\n",
    "\n",
    "prep.tokenize(data=dat.data)\n",
    "\n",
    "ids_ = [d.ID for d in prep.docs]\n",
    "arrs = [d.feature_array for d in prep.docs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tuple or struct_time argument required",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-1ff74992ae8e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0mtstamps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdatefmt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimestamp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0mtm\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlocal_time\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m     \u001B[0mtimes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmktime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m     \u001B[0mplatforms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplatform\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0mtokens\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\",\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_tokens_merged\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlowercase\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Tuple or struct_time argument required"
     ]
    }
   ],
   "source": [
    "import time\n",
    "df = DataFrame(arrs, index=ids_, columns=fields)\n",
    "tod_map = {}\n",
    "dates, times, tokens = [], [], []\n",
    "platforms = []\n",
    "tstamps = []\n",
    "for d in prep.docs:\n",
    "    datefmt = datetime.strptime(d.local_date, \"%Y-%m-%d\")\n",
    "    dates.append(datefmt)\n",
    "    tstamps.append(datefmt.timestamp())\n",
    "    tm = d.local_time\n",
    "    times.append(time.mktime(tm))\n",
    "    platforms.append(d.platform)\n",
    "    tokens.append(\",\".join(d.get_tokens_merged(lowercase=False)))\n",
    "df.insert(0, \"tokens\", tokens)\n",
    "df.insert(0, 'date', dates)\n",
    "df.insert(0, 'time', times)\n",
    "df.insert(0, 'platform', platforms)\n",
    "df.insert(0, 'date_ts', tstamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pkg.utils import make_tokens_list\n",
    "from tabulate import tabulate\n",
    "\n",
    "all_tokens = make_tokens_list(df['tokens'])\n",
    "android_tokens_series = df.loc[[i for i in df.index\n",
    "                                if df.loc[i]['platform']=='android']]['tokens']\n",
    "android_tokens = make_tokens_list(android_tokens_series)\n",
    "iphone_tokens_series = df.loc[[i for i in df.index\n",
    "                               if df.loc[i]['platform']=='iphone']]['tokens']\n",
    "iphone_tokens = make_tokens_list(iphone_tokens_series)\n",
    "\n",
    "\n",
    "words_counter = dict(Counter(all_tokens).most_common())\n",
    "android_counter = dict(Counter(android_tokens).most_common())\n",
    "iphone_counter = dict(Counter(iphone_tokens).most_common())\n",
    "\n",
    "\n",
    "android_words_unique = [(k, android_counter[k]) for k in android_counter.keys()\n",
    "                        if k not in iphone_counter]\n",
    "\n",
    "iphone_words_unique = [(kx, iphone_counter[kx]) for kx in iphone_counter.keys()\n",
    "                       if kx not in android_counter]\n",
    "\n",
    "sidexside = [(*android_words_unique[ix], *iphone_words_unique[ix]) for ix in range(20)]\n",
    "combined_table = tabulate(sidexside, headers=(\"android_token\", \"android_count\", 'iphone_token', 'iphone_count'))\n",
    "\n",
    "print(combined_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Difference Between IPhone and Android\n",
    "\n",
    "Its clear just from looking at the most common tokens that\n",
    "the posts originating from Android are more \"incendiary\" in terms\n",
    "of the insulting tone of many keywords. While this is certainly a characteristic\n",
    "one might associate with Trump it is not enough by itself to support the\n",
    "contention that Trump wrote all of these himself.\n",
    "\n",
    "Ironically, it is the tokens from the IPhone group which discuss the \"fake news\"\n",
    "in great detail. Though this makes sense if you consider that Trump switched to an\n",
    "IPhone after taking office. So we expand our condition to include certain times of the day\n",
    "in 2018 when he was suspected of tweeting and compare the counts again."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "trump = df.loc[[ix for ix in df.index if (13 >= time.strptime(df.loc[ix]['time'], \"%H:%M:%S\").tm_hour >= 10\n",
    "                                           and df.loc[ix]['date'].year == 2018\n",
    "                                          and df.loc[ix]['platform']=='iphone')\n",
    "                or df.loc[ix]['platform'] == 'android']]['tokens']\n",
    "\n",
    "trump_tokens = make_tokens_list(trump)\n",
    "not_trump = df.loc[[ix for ix in df.index if (time.strptime(df.loc[ix]['time'], \"%H:%M:%S\").tm_hour > 13 or time.strptime(df.loc[ix]['time'], \"%H:%M:%S\").tm_hour < 10)\n",
    "                                           and df.loc[ix]['date'].year == 2018\n",
    "                                          and df.loc[ix]['platform']=='iphone']]['tokens']\n",
    "\n",
    "not_trump_tokens = make_tokens_list(not_trump)\n",
    "\n",
    "trump_counter = dict(Counter(trump_tokens).most_common())\n",
    "not_trump_counter = dict(Counter(not_trump_tokens).most_common())\n",
    "trump_unique = [(tk, trump_counter[tk]) for tk in trump_counter if tk not in not_trump_counter]\n",
    "not_trump_unique = [(ntk, not_trump_counter[ntk]) for ntk in not_trump_counter if ntk not in trump_counter]\n",
    "\n",
    "\n",
    "\n",
    "sidexside2 = [(*trump_unique[ix], *not_trump_unique[ix]) for ix in range(min(len(trump_unique), len(not_trump_unique), 20))]\n",
    "combined_table2 = tabulate(sidexside2, headers=(\"trump_token\", \"trump_count\", 'nottrump_token', 'nottrump_count'))\n",
    "\n",
    "print(combined_table2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There were several things I noticed from the updated tokens lists:\n",
    "\n",
    "    (1) The 'likely' Trump tokens are shorter on average.\n",
    "    (2) The 'likely' Trump tokens are more sentiment oriented.\n",
    "    (3) The 'unlikely' Trump tokens have more numeric values.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linguistic Feature Groups"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mod0 = cluster(data=arrs, cluster_meth='optics', xi=.00075,\n",
    "               metric='l2', min_cluster_size=300)\n",
    "\n",
    "mod0_labs = mod0.labels_\n",
    "print(len(np.unique(mod0_labs)))\n",
    "print(dict(Counter(mod0_labs)))\n",
    "\n",
    "\n",
    "df.insert(len(df.columns), \"cluster\", mod0_labs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tod_map = {}\n",
    "dates, times, tokens = [], [], []\n",
    "\n",
    "\n",
    "clusts = df['cluster']\n",
    "\n",
    "outlier_filt = clusts == -1\n",
    "clust_filt = clusts != -1\n",
    "\n",
    "df_outliers = df.where(outlier_filt)\n",
    "df_clusts = df.where(clust_filt)\n",
    "df_outliers = df_outliers.dropna()\n",
    "df_clusts = df_clusts.dropna()\n",
    "grps = df_clusts.groupby(['cluster'])\n",
    "\n",
    "\n",
    "for grp in grps:\n",
    "    grpdf = grp[1]\n",
    "    grptokens = make_tokens_list(grpdf['tokens'])\n",
    "    grptoken_cnt = dict(Counter(grptokens).most_common(20)).items()\n",
    "    print(f\"\\n------------------\\nGroup #{int(grp[0])}\\n====================\\n\")\n",
    "    grptable = tabulate(grptoken_cnt, headers=('token', 'count'))\n",
    "\n",
    "    print(grptable)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combining filters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import strptime\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "pal = sns.color_palette(\"hls\", len(np.unique(mod0_labs))-1)\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "#fieldsx = fields.copy()\n",
    "#fieldsx.append('cluster')\n",
    "#fig = sns.pairplot(df[fieldsx], hue='cluster')\n",
    "\n",
    "#fig.savefig('plots/lang_attrs.png')\n",
    "\n",
    "# maybe_trump_df = df.loc[[idx for idx in df.index\n",
    "#                          if (df.loc[idx]['platform'] == 'android'\n",
    "#                              or df.loc[idx]['platform'] == 'web client')\n",
    "#                          or df.loc[idx]['date'] <=  datetime(2017, 1, 20)\n",
    "#                          or (df.loc[idx]['date'] > datetime(2017, 1, 20) and\n",
    "#                              13 > strptime(df.loc[idx]['time'], \"%H:%M:%S\").tm_hour > 10)\n",
    "#                          ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "Ultimately, it is evident that text features alone cannot serve as a substitute\n",
    "for observed patterns. However, it is interesting that the text features did segment\n",
    "the posts into what could be considered rough categories where each group seems to be\n",
    "centered on a theme. This would seem to indicate that, at least in this case, the linguistic\n",
    "patterns of posts share features when they are about the topic or related to a theme.\n",
    "\n",
    "Lastly, I have to give credit to ___ who wrote the article identifying the conditions\n",
    "of Trumps tweets. I find there is a significant degree of merit in their observations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pal = ['green','orange','dodgerblue']\n",
    "\n",
    "df_clusts['cluster'] = [int(x) for x in df_clusts['cluster']]\n",
    "funcfmt_date = lambda x, pos: datetime.fromtimestamp(x).strftime(\"%Y-%m\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 40), sharex=True, sharey=True)\n",
    "fig.suptitle(\"Tweet Clusters By Date\")\n",
    "sns.violinplot(ax=axes[0], x='date_ts', y='cluster', orient='h',\n",
    "               data=df_clusts, palette=pal, inner=None)\n",
    "\n",
    "sns.scatterplot(ax=axes[1], x='date_ts', y='cluster', data=df_clusts,\n",
    "                hue='cluster', palette=pal)\n",
    "\n",
    "\n",
    "axes[0].xaxis.set_major_formatter(funcfmt_date)\n",
    "axes[1].xaxis.set_major_formatter(funcfmt_date)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_clusts['time'] = [x for x in df_clusts['time']]\n",
    "funcfmt_time = lambda x, pos: datetime.fromtimestamp(x).strftime('%H:%M')\n",
    "\n",
    "\n",
    "fig0, axes0 = plt.subplots(2, 1, figsize=(10, 40), sharex=True, sharey=True)\n",
    "fig0.suptitle(\"Tweet Clusters By Time\")\n",
    "sns.violinplot(ax=axes0[0], x='time', y='cluster', orient='h',\n",
    "               data=df_clusts, palette=pal, inner=None)\n",
    "\n",
    "sns.scatterplot(ax=axes0[1], x='time', y='cluster', data=df_clusts,\n",
    "                hue='cluster', palette=pal)\n",
    "axes0[0].xaxis.set_major_formatter(funcfmt_time)\n",
    "axes0[1].xaxis.set_major_formatter(funcfmt_time)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-abb1a51f",
   "language": "python",
   "display_name": "PyCharm (receipts)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}